# 《大模型开发极简入门：基于GPT-4和chatGPT》学习笔记整理

## LLM概述

GPT-4和其他GPT模型是基于大量数据训练而成的大语言模型(large language model，LLM)，它们能够以非常高的准确性识别和生成人类可读的文本。

### 1.1.1 探索语言模型和NLP的基础

![image.png](%E3%80%8A%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BC%80%E5%8F%91%E6%9E%81%E7%AE%80%E5%85%A5%E9%97%A8%EF%BC%9A%E5%9F%BA%E4%BA%8EGPT-4%E5%92%8CchatGPT%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86%2062a9fe80b48f4a0cbd89556d64101be7/image.png)

作为LLM，GPT-4和ChatGPT是NLP领域中最新的模型类型，NLP是机器学习和人工智能的一个子领域。在深入研究GPT-4和ChatGPT之前，有必要了解NLP及其相关领域。

AI有不同的定义，但其中一个定义或多或少已成为共识，即AI是一类计算机系统，它能够执行通常需要人类智能才能完成的任务。

机器学习(machine learning，ML)是AI的一个子集。在ML中，我们不试图直接实现AI系统使用的决策规则。相反，我们试图开发算法，使系统能够通过示例自己学习。

在这些ML算法中，深度学习(deep learning，DL)算法已经引起了广泛关注。DL是ML的一个分支，专注于受大脑结构启发的算法。这些算法被称为人工神经网络(artificial neural network)。它们可以处理大量的数据，并且在图像识别、语音识别及NLP等任务上表现出色。

GPT-4和ChatGPT基于一种特定的神经网络架构，即Transformer。Transformer就像阅读机一样，它关注句子或段落的不同部分，以理解其上下文并产生连贯的回答。此外，它还可以理解句子中的单词顺序和上下文意思。这使Transformer在语言翻译、问题回答和文本生成等任务中非常有效。

LLM是试图完成文本生成任务的一类ML模型。

LLM的发展开始于简单的语言模型，如n-gram模型。n-gram模型通过使用词频来根据前面的词预测句子中的下一个词预测结果是在训练文本中紧随前面的词出现的频率最高的词。缺点是在理解上下文和语法方面还需要改进，它会生成不连贯的文本。

为了提高n-gram模型，引入了更先进的学习算法，包括循环神经网络(recurrent neural network，RNN)和长短期记忆(long short-term memory，LSTM)网络。与n-gram模型相比，这些模型能够学习更长的序列，并且能够更好地分析上下文，但它们在处理大量数据时的效率仍然欠佳。

### 1.1.2 理解Transformer架构及其在LLM中的作用（重点）

RNN存在一个关键性的问题：灾难性遗忘问题，即在处理长文本序列时容易忘记上下文。

Transformer架构彻底改变了NLP领域，这主要是因为它能够有效地解决这个问题。

这场革命的核心支柱是注意力机制，这是一个简单而又强大的机制。模型不再将文本序列中的所有词视为同等重要，而是在任务的每个步骤中关注最相关的词。交叉注意力和自注意力是基于注意力机制的两个架构模块，它们经常出现在LLM中。Transformer架构广泛使用了交叉注意力模块和自注意力模块。

交叉注意力有助于模型确定输入文本的不同部分与输出文本中下一个词的相关性。它就像一盏聚光灯，照亮输入文本中的词或短语，并突出显示预测下一个词所需的相关信息，同时忽略不重要的细节。

自注意力机制是指模型能够关注其输入文本的不同部分。具体到NLP领域，自注意力机制使模型能够评估句子中的每个词相比于其他词的重要性。这使得模型能够更好地理解各词之间的关系，并根据输入文本中的多个词构建新概念。

**ps：补充注意力机制的知识点：**

编码器和解码器的作用：

标准的Transformer架构有两个主要组件：编码器和解码器，两者都十分依赖注意力机制。编码器的任务是处理输入文本，识别有价值的特征，并生成有意义的文本表示，称为嵌入(embedding)。解码器使用这个嵌入来生成一个输出，比如翻译结果或摘要文本。这个输出有效地解释了编码信息。

GTP：生成式预训练Transformer(Generative Pre-trained Transformer）是一类基于Transformer架构的模型，专门利用原始架构中的解码器部分。在GPT中，不存在编码器，因此无须通过交叉注意力机制来整合编码器产生的嵌入。也就是说，GPT仅依赖解码器内部的自注意力机制来生成上下文感知的表示和预测结果。

![image.png](%E3%80%8A%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BC%80%E5%8F%91%E6%9E%81%E7%AE%80%E5%85%A5%E9%97%A8%EF%BC%9A%E5%9F%BA%E4%BA%8EGPT-4%E5%92%8CchatGPT%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86%2062a9fe80b48f4a0cbd89556d64101be7/image%201.png)

GPT收到一段提示词之后，首先将输入拆分成token，这个是分词器的功用。然后生成一段文本。与之前的循环模型不同，带有注意力机制的Transformer架构使得LLM能够将上下文作为一个整体来考虑。

### 1.4警惕AI幻觉

正如你所见，LLM根据给定的输入提示词逐个预测下一个词（也就是标记），从而生成回答。在大多数情况下，模型的输出是与提问相关的，并且完全可用，但是在使用语言模型时需要小心，因为它们给出的回答可能不准确。这种回答通常被称为AI幻觉，即AI自信地给出一个回答，但是这个回答是错误的，或者涉及虚构的信息。

ChatGPT和GPT-4在设计上并不可靠：它们可能会提供错误信息，甚至误导用户。

### 2 深入了解GPT-4和ChatGPT的API

![image.png](%E3%80%8A%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BC%80%E5%8F%91%E6%9E%81%E7%AE%80%E5%85%A5%E9%97%A8%EF%BC%9A%E5%9F%BA%E4%BA%8EGPT-4%E5%92%8CchatGPT%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86%2062a9fe80b48f4a0cbd89556d64101be7/image%202.png)

```python
import openai
# 对GPT-3.5 Turbo来说，端点是ChatCompletion
openai.ChatCompletion.create(
    # 对GPT-3.5 Turbo来说，模型是gpt-3.5-turbo
    model="gpt-3.5-turbo",
    # 消息列表形式的对话
    messages=[
        {"role": "system", "content": "You are a helpful teacher."},
        {
            "role": "user",
            "content": "Are there other measures than time \
            complexity for an algorithm?",
        },
        {
            "role": "assistant",
            "content": "Yes, there are other measures besides time \
            complexity for an algorithm, such as space complexity.",
        },
        {"role": "user", "content": "What is it?"},
    ],
)
```

### 2.8 嵌入

在ML领域，特别是在处理语言模型时，我们会遇到嵌入这一重要概念。嵌入将分类数据（比如标记，通常是单个词或多组标记）转换为数值格式，具体而言是实数向量。这种转换是必要的，因为ML模型依赖数值数据，其直接处理分类数据的能力欠佳。

可以将嵌入视为一种复杂的语言解释器，它将丰富的词汇和句子转换为ML模型能够轻松理解的数值语言。嵌入的一个突出特点是，它能够保持语义相似性。也就是说，含义相近的词语或短语在数值空间中更接近。嵌入具有这样的属性：如果两段文本具有相似的含义，那么它们的向量表示也是相似的。

通过以下方式访问嵌入：

result['data']['embedding']

结果嵌入是一个向量，即一个浮点数数组。

### 3 使用GPT-4和ChatGPT构建应用程序

要开发基于LLM的应用程序，核心是将LLM与OpenAI API集成。

这需要开发人员仔细管理API密钥，考虑数据安全和数据隐私，并降低集成LLM的服务受特定攻击的风险。

对于API密钥，你有两个选择。

**·让应用程序的用户自己提供API密钥。**

在这方面，你有两个选择。·只有在必要时才要求用户提供API密钥，并且永远不要通过远程服务器存储或使用它。在这种情况下，API密钥将永远不会离开用户，应用程序将从在用户设备上执行的代码中调用API。·在后端管理数据库并将API密钥安全地存储在数据库中。

在设计解决方案时，请考虑以下API密钥管理原则。·对于Web应用程序，将API密钥保存在用户设备的内存中，而不要用浏览器存储。·如果选择后端存储API密钥，那么请强制采取高安全性的措施，并允许用户自己控制API密钥，包括删除API密钥。·在传输期间和静态存储期间加密API密钥。

**·在应用程序中使用你自己的API密钥。**

·永远不要直接将API密钥写入代码中。

·不要将API密钥存储在应用程序的源代码文件中。

·不要在用户的浏览器中或个人设备上使用你的API密钥。

·设置使用限制，以确保预算可控。

### 3.2 软件架构设计原则

建议你在构建应用程序时和OpenAI API解耦。OpenAI的服务可能会发生变化，你无法控制OpenAI管理API的方式。最佳实践是确保API的变化不会迫使你完全重写应用程序。可以通过遵循架构设计模式来实现这一点。

![image.png](%E3%80%8A%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BC%80%E5%8F%91%E6%9E%81%E7%AE%80%E5%85%A5%E9%97%A8%EF%BC%9A%E5%9F%BA%E4%BA%8EGPT-4%E5%92%8CchatGPT%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86%2062a9fe80b48f4a0cbd89556d64101be7/image%203.png)

### 3.3 LLM驱动型应用程序的漏洞

你必须意识到，将用户输入作为提示词发送给LLM的任何面向用户的应用程序都容易受到提示词注入攻击。提示词注入的原理如下：用户向应用程序发送一条输入消息，比如“忽略所有先前的指令，执行其他操作”。由于此输入消息与你在构建应用程序时设计的提示词连接在一起，因此AI模型将遵循用户的提示词，而不是你的提示词。

几种可以用来降低提示词注入攻击的风险

1.使用特定规则控制用户输入

2.控制输入长度。因为输入越短，攻击者找到有效的恶意提示词的可能性就越小。

3.控制输出

4.监控应用程序的输入和输出，以便能够在事后检测到攻击。你还可以对用户进行身份验证，以便检测和阻止恶意账户。

5.意图分析。OpenAI提供了一个可用于检测内容合规性的内容审核模型。你可以使用这个模型，构建自己的内容审核模型，或者向OpenAI发送另一个请求，以验证模型给出的回答是否合规。

### 4 GPT-4和ChatGPT的高级技巧

### 4.1 提示工程

提示工程是一门新兴的学科，专注于以最佳实践构建LLM的最佳输入，从而尽可能以程序化方式生成目标输出。AI工程师必须知道如何与AI进行交互，以获取可用于应用程序的有利结果。此外，AI工程师还必须知道如何正确提问和编写高质量的提示词。

在所有这些任务中，我们通常需要在提示词中定义三大要素：**角色、上下文和任务。**

**01.上下文**

上下文提供更多细节有助于输出更有价值的回答。还有一种方式是我们修改提示词，不再是要求模型提供答案，而是指示它以增强补全效果为目标来向我们提出关于上下文的问题。

**02.任务**

任务定义了你对GPT-4模型的用法，并且应该明确且具体。你应该提供足够的任务信息，并在提示词中使用合适的短语来引导模型给出你所期望的结果。

**03.角色**

撰写提示词时可以给模型赋予一个角色，比如赋予运动营养专家的角色。

在提示词的末尾添加“让我们逐步思考”这样的话，已被证明可以使模型解决更复杂的推理问题。这种技术称为零样本思维链策略(zero-shot-CoTstrategy)。思维链是指使用提示词鼓励模型逐步模仿推理的技术。零样本这个术语意味着模型不依赖于特定任务的示例来执行这种推理，它已经准备好根据其一般的训练结果来处理新任务。与我们稍后就会讨论的少样本学习等技术不同，零样本学习试图在不需要特定任务示例的情况下进行泛化。

少样本学习(few-shot learning)是由Tom B.Brown等人在论文“Language Models Are Few-Shot Learners”中提出的，它指的是LLM仅通过提示词中的几个示例就能进行概括并给出有价值的结果。我们可以看到，仅凭几个示例，模型就能够复现模式。通过利用在训练阶段所获得的海量知识，LLM可以根据少量例子迅速适应并生成准确的答案。 少样本学习是LLM的一个强大的特点，因为它使得LLM高度灵活且适应性强，只需有限的额外信息即可执行各种任务。

指导LLM的另一种方法是单样本学习(one-shotlearning)。顾名思义，在单样本学习中，我们只提供一个示例来帮助模型执行任务。尽管这种方法提供的指导比少样本学习要少，但对于简单的任务或LLM已经具备丰富背景知识的主题，它可能很有效。单样本学习的优点是更简单、生成速度更快、计算成本更低（因而API使用成本更低）。然而，对于复杂的任务或需要更深入理解所需结果的情况，少样本学习的效果可能更好。

### 4.2 微调

OpenAI提供了许多可直接使用的GPT模型。尽管这些模型在各种任务上表现出色，但针对特定任务或上下文对它们进行微调，可以进一步提高它们的性能。

![image.png](%E3%80%8A%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BC%80%E5%8F%91%E6%9E%81%E7%AE%80%E5%85%A5%E9%97%A8%EF%BC%9A%E5%9F%BA%E4%BA%8EGPT-4%E5%92%8CchatGPT%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86%2062a9fe80b48f4a0cbd89556d64101be7/image%204.png)

**02.对比微调和少样本学习**

微调是指针对特定任务在一组数据上重新训练现有模型，以提高模型的性能并使其回答更准确。在微调过程中，模型的内部参数得到更新。少样本学习则是通过提示词向模型提供有限数量的好例子，以指导模型根据这些例子给出目标结果。在少样本学习过程中，模型的内部参数不会被修改。

少样本学习是一种更灵活的方法，其数据使用率也更高，因为它不需要重新训练模型。当只有有限的示例可用或需要快速适应不同任务时，这种技巧非常有益。少样本学习让开发人员能够快速设计原型并尝试各种任务，这使其成为许多用例的实用选择。这两种方法的另一个关键选择标准是成本，毕竟使用和训练微调模型更贵。对于微调任务，我们通常建议提供几百个例子，最好提供几千个。

### 5 使用LangChain框架和插件增强LLM的功能

LangChain是专用于开发LLM驱动型应用程序的框架。

![image.png](%E3%80%8A%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BC%80%E5%8F%91%E6%9E%81%E7%AE%80%E5%85%A5%E9%97%A8%EF%BC%9A%E5%9F%BA%E4%BA%8EGPT-4%E5%92%8CchatGPT%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86%2062a9fe80b48f4a0cbd89556d64101be7/image%205.png)

LangChain的一个特点是动态提示词。PromptsTemplate负责构建模型的输入。也就是说，它能以可复制的方式生成提示词。它包含一个

智能体及工具是LangChain框架提供的关键功能：它们可以使应用程序变得非常强大，让LLM能够执行各种操作并与各种功能集成，从而解决复杂的问题。智能体及工具是LangChain框架提供的关键功能：它们可以使应用程序变得非常强大，让LLM能够执行各种操作并与各种功能集成，从而解决复杂的问题。

![image.png](%E3%80%8A%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BC%80%E5%8F%91%E6%9E%81%E7%AE%80%E5%85%A5%E9%97%A8%EF%BC%9A%E5%9F%BA%E4%BA%8EGPT-4%E5%92%8CchatGPT%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86%2062a9fe80b48f4a0cbd89556d64101be7/image%206.png)

在某些应用程序中，记住之前的交互是至关重要的，无论是短期记忆还是长期记忆。使用LangChain，你可以轻松地为链和智能体添加状态以管理记忆。构建聊天机器人是这种能力最常见的用例。

### 5.2 GPT-4插件

尽管包括GPT-4在内的LLM在各种任务上都表现出色，但它们仍然存在固有的局限性。比如，这些模型只能从训练数据中学习，这些数据往往过时或不适用于特定的应用。此外，它们的能力仅限于文本生成。我们还发现，LLM不适用于某些任务，比如复杂的计算任务。这就引出了GPT-4的一个开创性特点：插件。在AI的发展过程中，插件已经成为一种新型的革命性工具，它重新定义了我们与LLM的互动方式。插件的目标是为LLM提供更广泛的功能，使LLM能够访问实时信息，进行复杂的数学运算，并利用第三方服务。

LangChain的真正潜力在于创造性地利用各种功能来解决复杂问题，并将通用语言模型转化为功能强大且具体的应用程序。